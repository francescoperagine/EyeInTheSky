{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install loguru==0.7.3 python-dotenv==1.0.1 PyYAML==6.0.2 torch==2.5.1 tqdm==4.67.1 typer==0.15.1 matplotlib==3.10.0 pyarrow==18.1.0 setuptools==75.1.0 protobuf==4.25.3 ultralytics ray==2.43.0 albumentations==2.0.5 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eyeinthesky.utils import *\n",
    "import yaml\n",
    "from ultralytics import YOLO, settings\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer, DetectionValidator\n",
    "from ultralytics.utils import colorstr, LOGGER, DEFAULT_CFG\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ultralytics.utils.loss import v8DetectionLoss\n",
    "from ultralytics.cfg import get_cfg\n",
    "import torch.nn.functional as F\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "config_data = \"\"\"\n",
    "wandb:\n",
    "  project: \"EyeInTheSky_merged\"\n",
    "  group: \"distillation\"\n",
    "data: \"VisDrone.yaml\"\n",
    "k_samples: 5\n",
    "distillation:\n",
    "  teacher_model: \"yolo12m.pt\"\n",
    "  temperature: 2.0\n",
    "  alpha: 0.5\n",
    "train:\n",
    "  project: \"EyeInTheSky\"\n",
    "  data: \"VisDrone.yaml\"\n",
    "  pretrained: True\n",
    "  patience: 5\n",
    "  task: detect\n",
    "  epochs: 500\n",
    "  seed: 42\n",
    "  plots: True\n",
    "  exist_ok: False\n",
    "  save: True\n",
    "  save_period: 10\n",
    "  val: True\n",
    "  warmup_epochs: 10\n",
    "  visualize: True\n",
    "  show: True\n",
    "  single_cls: False\n",
    "  rect: False\n",
    "  resume: False\n",
    "  fraction: 1.0\n",
    "  freeze: None\n",
    "  cache: False\n",
    "  verbose: False\n",
    "  amp: True\n",
    "val:\n",
    "  project: \"EyeInTheSky\"\n",
    "  half: True\n",
    "  conf: 0.25\n",
    "  iou: 0.6\n",
    "  split: \"test\"\n",
    "  rect: True\n",
    "  plots: True\n",
    "  visualize: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device\n",
    "\n",
    "def get_device() -> str:\n",
    "    try:\n",
    "        return 0 if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting device: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "\n",
    "# config = Config.load(\"../config/config.yaml\")\n",
    "config = yaml.safe_load(config_data)\n",
    "config[\"train\"].update({\"device\" : get_device()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wandb\n",
    "\n",
    "# def wandb_start(key, run_config, wandb_config):\n",
    "#     settings.update({\"wandb\": True})\n",
    "#     if wandb.run is None: \n",
    "#         wandb.login(key=key, relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, Trainer, Validator\n",
    "\n",
    "class VisDroneDataset(YOLODataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for VisDrone that merges pedestrian (0) and people (1) classes.\n",
    "    Handles class remapping at the earliest possible stage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the merged names as a class attribute to be accessible from the trainer\n",
    "    merged_names = {\n",
    "        0: 'persona',\n",
    "        1: 'bicicletta',\n",
    "        2: 'auto',\n",
    "        3: 'furgone',\n",
    "        4: 'camion',\n",
    "        5: 'triciclo',\n",
    "        6: 'triciclo-tendato',\n",
    "        7: 'autobus',\n",
    "        8: 'motociclo'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Initialize parent class with modified kwargs\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Log class mapping\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Using merged classes: {self.merged_names}\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"\n",
    "        Load and process labels with class remapping.\n",
    "        \"\"\"\n",
    "        # Get labels from parent method\n",
    "        labels = super().get_labels()\n",
    "        \n",
    "        # Process statistics\n",
    "        people_count = 0\n",
    "        shifted_count = 0\n",
    "        \n",
    "        # Process labels to merge classes\n",
    "        for i in range(len(labels)):\n",
    "            cls = labels[i]['cls']\n",
    "            \n",
    "            if len(cls) > 0:\n",
    "                # Count 'people' instances\n",
    "                people_mask = cls == 1\n",
    "                people_count += np.sum(people_mask)\n",
    "                \n",
    "                # Merge class 1 (people) into class 0 (pedestrian -> person)\n",
    "                cls[people_mask] = 0\n",
    "                \n",
    "                # Shift classes > 1 down by 1\n",
    "                gt1_mask = cls > 1\n",
    "                shifted_count += np.sum(gt1_mask)\n",
    "                cls[gt1_mask] -= 1\n",
    "                \n",
    "                # Store modified labels\n",
    "                labels[i]['cls'] = cls\n",
    "        \n",
    "        # Now set correct class count and names for training\n",
    "        if hasattr(self, 'data'):\n",
    "            # Update names and class count\n",
    "            self.data['names'] = self.merged_names\n",
    "            self.data['nc'] = len(self.merged_names)\n",
    "        \n",
    "        # Log statistics\n",
    "        person_count = sum(np.sum(label['cls'] == 0) for label in labels)\n",
    "        LOGGER.info(f\"\\n{colorstr('VisDroneDataset:')} Remapped {people_count} 'people' instances to {self.merged_names[0]}\")\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Total 'persona' instances after merge: {person_count}\")\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Shifted {shifted_count} instances of other classes\")\n",
    "        \n",
    "        return labels\n",
    "\n",
    "class MergedClassDetectionTrainer(DetectionTrainer):\n",
    "    \"\"\"\n",
    "    Custom trainer that uses VisDroneDataset for merged class training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_dataset(self, img_path, mode=\"train\", batch=None):\n",
    "        \"\"\"Build custom VisDroneDataset.\"\"\"\n",
    "        return VisDroneDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch or self.batch_size,\n",
    "            augment=mode == \"train\",\n",
    "            hyp=self.args,\n",
    "            rect=self.args.rect if mode == \"train\" else True,\n",
    "            cache=self.args.cache or None,\n",
    "            single_cls=self.args.single_cls,\n",
    "            stride=self.stride,\n",
    "            pad=0.0 if mode == \"train\" else 0.5,\n",
    "            prefix=colorstr(f\"{mode}: \"),\n",
    "            task=self.args.task,\n",
    "            classes=None,\n",
    "            data=self.data,\n",
    "            fraction=self.args.fraction if mode == \"train\" else 1.0,\n",
    "        )\n",
    "    \n",
    "    def set_model_attributes(self):\n",
    "        \"\"\"Update model attributes for merged classes.\"\"\"\n",
    "        # First call parent method to set standard attributes\n",
    "        super().set_model_attributes()\n",
    "        \n",
    "        # Then update model with the merged class names\n",
    "        if hasattr(self.model, 'names'):\n",
    "            # Use the merged names directly from the dataset class\n",
    "            self.model.names = VisDroneDataset.merged_names\n",
    "            self.model.nc = len(VisDroneDataset.merged_names)\n",
    "            \n",
    "            # Also update data dictionary\n",
    "            if hasattr(self, 'data'):\n",
    "                self.data['names'] = VisDroneDataset.merged_names\n",
    "                self.data['nc'] = len(VisDroneDataset.merged_names)\n",
    "\n",
    "class MergedClassDetectionValidator(DetectionValidator):\n",
    "    \"\"\"\n",
    "    Custom validator that uses VisDroneDataset for validation/testing with merged classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_dataset(self, img_path, mode=\"val\", batch=None):\n",
    "        \"\"\"Build custom VisDroneDataset for validation.\"\"\"\n",
    "        return VisDroneDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch or self.args.batch,\n",
    "            augment=False,\n",
    "            hyp=self.args,\n",
    "            rect=True,\n",
    "            cache=None,\n",
    "            single_cls=self.args.single_cls,\n",
    "            stride=self.stride,\n",
    "            pad=0.5,\n",
    "            prefix=colorstr(f\"{mode}: \"),\n",
    "            task=self.args.task,\n",
    "            classes=self.args.classes,\n",
    "            data=self.data,\n",
    "        )\n",
    "    \n",
    "    def set_model_attributes(self):\n",
    "        \"\"\"Update model attributes for merged classes if using a PyTorch model.\"\"\"\n",
    "        super().set_model_attributes()\n",
    "        \n",
    "        # Update model names if it's a PyTorch model (not for exported models)\n",
    "        if hasattr(self.model, 'names') and hasattr(self.model, 'model'):\n",
    "            self.model.names = VisDroneDataset.merged_names\n",
    "            if hasattr(self.data, 'names'):\n",
    "                self.data['names'] = VisDroneDataset.merged_names\n",
    "                self.data['nc'] = len(VisDroneDataset.merged_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DistillationLoss(v8DetectionLoss):\n",
    "#     def __init__(self, model, temperature=2.0, alpha=0.5, tal_topk=10):\n",
    "#         \"\"\"Initialize the distillation loss with model parameters.\"\"\"\n",
    "#         super().__init__(model, tal_topk=tal_topk)\n",
    "#         self.teacher_model = model.teacher_model\n",
    "#         self.temperature = temperature\n",
    "#         self.alpha = alpha\n",
    "#         self.step = 0\n",
    "\n",
    "#     def __call__(self, preds, batch):\n",
    "#         \"\"\"Calculate original detection loss and distillation loss.\"\"\"\n",
    "#         # Calculate standard detection loss using parent class\n",
    "#         original_loss, loss_items = super().__call__(preds, batch)\n",
    "        \n",
    "#         # Initialize distillation loss as zero tensor on same device\n",
    "#         distillation_loss = torch.tensor(0.0, device=original_loss.device)\n",
    "        \n",
    "#         # Only calculate distillation loss if teacher model exists\n",
    "#         if self.teacher_model is not None:\n",
    "#             with torch.no_grad():\n",
    "#                 # Get teacher predictions\n",
    "#                 self.teacher_model.eval()\n",
    "#                 teacher_preds = self.teacher_model(batch[\"img\"])\n",
    "            \n",
    "#             # Get feature maps for distillation\n",
    "#             student_feats = preds[1] if isinstance(preds, tuple) else preds\n",
    "#             teacher_feats = teacher_preds[1] if isinstance(teacher_preds, tuple) else teacher_preds\n",
    "            \n",
    "#             # Calculate distillation loss on each feature map\n",
    "#             for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "#                 # Apply temperature scaling\n",
    "#                 s_logits = s_feat / self.temperature\n",
    "#                 t_logits = t_feat / self.temperature\n",
    "                \n",
    "#                 # Apply log softmax to student and softmax to teacher\n",
    "#                 # Flatten from channel dimension for KL div calculation\n",
    "#                 s_log_prob = F.log_softmax(s_logits.flatten(2), dim=-1)\n",
    "#                 t_prob = F.softmax(t_logits.flatten(2), dim=-1)\n",
    "                \n",
    "#                 # KL divergence loss\n",
    "#                 feat_loss = F.kl_div(s_log_prob, t_prob, reduction='batchmean')\n",
    "#                 distillation_loss += feat_loss * (self.temperature**2)  # Scale by temperature squared\n",
    "        \n",
    "#         # Combine losses using alpha weighting\n",
    "#         combined_loss = (1 - self.alpha) * original_loss + self.alpha * distillation_loss\n",
    "        \n",
    "#         # # Increment step counter for consistent logging\n",
    "#         # self.step += 1\n",
    "        \n",
    "#         if wandb.run is not None:\n",
    "#             metrics = {\n",
    "#                 \"distill/combined_loss\": combined_loss.item(),\n",
    "#                 \"distill/detection_loss\": original_loss.item(),\n",
    "#                 \"distill/distillation_loss\": distillation_loss.item(),\n",
    "#                 \"distill/box\": loss_items[0].item(),\n",
    "#                 \"distill/cls\": loss_items[1].item(),\n",
    "#                 \"distill/dfl\": loss_items[2].item() if len(loss_items) > 2 else 0,\n",
    "#                 \"step\": self.step  # or use epoch if thatâ€™s more appropriate\n",
    "#             }\n",
    "#             wandb.log(metrics, step=self.step)\n",
    "        \n",
    "#         return combined_loss, loss_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(v8DetectionLoss):\n",
    "    def __init__(self, model, temperature=2.0, alpha=0.5, tal_topk=10):\n",
    "        super().__init__(model, tal_topk=tal_topk)\n",
    "        self.teacher_model = model.teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        # Accumulators for epoch metrics\n",
    "        self.epoch_metrics = {\"combined_loss\": 0.0, \"detection_loss\": 0.0, \"distillation_loss\": 0.0, \"count\": 1}\n",
    "\n",
    "    def __call__(self, preds, batch):\n",
    "        # Compute standard detection loss using the parent class\n",
    "        original_loss, loss_items = super().__call__(preds, batch)\n",
    "        distillation_loss = torch.tensor(0.0, device=original_loss.device)\n",
    "        \n",
    "        if self.teacher_model is not None:\n",
    "            with torch.no_grad():\n",
    "                self.teacher_model.eval()\n",
    "                teacher_preds = self.teacher_model(batch[\"img\"])\n",
    "            \n",
    "            # Get feature maps for distillation\n",
    "            student_feats = preds[1] if isinstance(preds, tuple) else preds\n",
    "            teacher_feats = teacher_preds[1] if isinstance(teacher_preds, tuple) else teacher_preds\n",
    "\n",
    "            for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "                s_logits = s_feat / self.temperature\n",
    "                t_logits = t_feat / self.temperature\n",
    "                s_log_prob = F.log_softmax(s_logits.flatten(2), dim=-1)\n",
    "                t_prob = F.softmax(t_logits.flatten(2), dim=-1)\n",
    "                feat_loss = F.kl_div(s_log_prob, t_prob, reduction='batchmean')\n",
    "                distillation_loss += feat_loss * (self.temperature**2)\n",
    "        \n",
    "        combined_loss = (1 - self.alpha) * original_loss + self.alpha * distillation_loss\n",
    "\n",
    "        # Accumulate losses for this epoch\n",
    "        self.epoch_metrics[\"combined_loss\"] += combined_loss.item()\n",
    "        self.epoch_metrics[\"detection_loss\"] += original_loss.item()\n",
    "        self.epoch_metrics[\"distillation_loss\"] += distillation_loss.item()\n",
    "        self.epoch_metrics[\"count\"] += 1\n",
    "\n",
    "        return combined_loss, loss_items\n",
    "\n",
    "    # def log_epoch_metrics(self, epoch):\n",
    "    #     if self.epoch_metrics[\"count\"] > 0:\n",
    "    #         avg_combined = self.epoch_metrics[\"combined_loss\"] / self.epoch_metrics[\"count\"]\n",
    "    #         avg_detection = self.epoch_metrics[\"detection_loss\"] / self.epoch_metrics[\"count\"]\n",
    "    #         avg_distill = self.epoch_metrics[\"distillation_loss\"] / self.epoch_metrics[\"count\"]\n",
    "\n",
    "    #         metrics = {\n",
    "    #             \"distill/avg_combined_loss\": avg_combined,\n",
    "    #             \"distill/avg_detection_loss\": avg_detection,\n",
    "    #             \"distill/avg_distillation_loss\": avg_distill,\n",
    "    #             \"epoch\": epoch\n",
    "    #         }\n",
    "    #         wandb.log(metrics, step=epoch)\n",
    "\n",
    "        # Reset accumulators for the next epoch\n",
    "        self.epoch_metrics = {\"combined_loss\": 0.0, \"detection_loss\": 0.0, \"distillation_loss\": 0.0, \"count\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationModel(DetectionModel):\n",
    "    def __init__(self, cfg='yolov12n.yaml', ch=3, nc=None, verbose=True, teacher_model=None, temperature=1.0, alpha=0.5):\n",
    "        \"\"\"Initialize distillation model with teacher model for knowledge transfer.\"\"\"\n",
    "        super().__init__(cfg, ch, nc, verbose)\n",
    "        \n",
    "        self.teacher_model = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Freeze teacher model\n",
    "        # if self.teacher_model is not None:\n",
    "        #     self.teacher_model.eval()\n",
    "        #     for param in self.teacher_model.parameters():\n",
    "        #         param.requires_grad = False\n",
    "            \n",
    "        #     # Ensure teacher has same class names as student\n",
    "        #     if hasattr(self, 'names') and hasattr(self.teacher_model, 'names'):\n",
    "        #         self.teacher_model.names = self.names\n",
    "            \n",
    "        LOGGER.info(f\"Initialized DistillationModel with temperature={self.temperature}, alpha={self.alpha}\")\n",
    "\n",
    "    def init_criterion(self):\n",
    "        \"\"\"Initialize the custom distillation loss criterion.\"\"\"\n",
    "        return DistillationLoss(self, temperature=self.temperature, alpha=self.alpha)\n",
    "        \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"Forward pass with optional teacher model evaluation.\"\"\"\n",
    "        # Standard forward pass\n",
    "        result = super().forward(x, *args, **kwargs)\n",
    "        \n",
    "        # For validation, also get teacher predictions if requested\n",
    "        if not self.training and kwargs.get('teacher_eval', False) and self.teacher_model is not None:\n",
    "            with torch.no_grad():\n",
    "                teacher_result = self.teacher_model(x, *args, **kwargs)\n",
    "                # Return both student and teacher results\n",
    "                return {'student': result, 'teacher': teacher_result}\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(MergedClassDetectionTrainer):\n",
    "    distillation_config = None\n",
    "    teacher_model_dir = None\n",
    "    temperature = 1.0\n",
    "    alpha = 0.5\n",
    "\n",
    "    @staticmethod\n",
    "    def set_config(distill_config):\n",
    "        DistillationTrainer.distillation_config = distill_config\n",
    "\n",
    "    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n",
    "        \"\"\"Initialize distillation trainer with teacher model.\"\"\"\n",
    "        if DistillationTrainer.distillation_config is not None:\n",
    "            self.distillation_config = DistillationTrainer.distillation_config\n",
    "            self.teacher_model_dir = DistillationTrainer.distillation_config[\"teacher_model\"]\n",
    "            self.temperature = DistillationTrainer.distillation_config[\"temperature\"]\n",
    "            self.alpha = DistillationTrainer.distillation_config[\"alpha\"]\n",
    "\n",
    "        self.args = get_cfg(cfg, overrides)\n",
    "        super().__init__(cfg, overrides, _callbacks)\n",
    "\n",
    "        self.teacher_model = self._setup_teacher_model()\n",
    "\n",
    "        overrides['freeze'] = 21\n",
    "        self.teacher_model.args = get_cfg(cfg, overrides)\n",
    "        \n",
    "        # self.add_callback(\"on_train_epoch_end\", self.log_training_metrics)\n",
    "        # self.add_callback(\"on_val_end\", self.log_validation_metrics)\n",
    "        # self.add_callback(\"on_val_end\", self.log_teacher_metrics)\n",
    "        self.add_callback(\"on_train_epoch_end\", self.log_distillation_metrics)\n",
    "\n",
    "    def _setup_teacher_model(self):\n",
    "        \"\"\"Load teacher model from checkpoint.\"\"\"\n",
    "        print(f\"Loading teacher model from {self.teacher_model_dir}\")\n",
    "        \n",
    "        teacher = YOLO(self.teacher_model_dir)\n",
    "        teacher_model = teacher.model\n",
    "        teacher_model.names = VisDroneDataset.merged_names\n",
    "        \n",
    "        return teacher_model\n",
    "\n",
    "    def get_model(self, cfg=None, weights=None, verbose=True):\n",
    "        \"\"\"Create and return a DistillationModel with teacher model.\"\"\"\n",
    "        model = DistillationModel(\n",
    "            cfg=cfg, \n",
    "            nc=self.data[\"nc\"],\n",
    "            verbose=verbose,\n",
    "            teacher_model=self.teacher_model,\n",
    "            temperature=self.temperature, \n",
    "            alpha=self.alpha\n",
    "        )\n",
    "\n",
    "        model.args = self.args\n",
    "        \n",
    "        if weights:\n",
    "            model.load(weights)\n",
    "            \n",
    "        return model        \n",
    "\n",
    "    def _setup_train(self, world_size):\n",
    "        \"\"\"Override parent method to handle teacher model freezing correctly\"\"\"\n",
    "        # Call parent method to set up most things\n",
    "        super()._setup_train(world_size)\n",
    "        \n",
    "        self.teacher_model.eval()\n",
    "\n",
    "        print(f\"self.teacher_model.parameters() {self.teacher_model.parameters()}\")\n",
    "        # Now ensure the teacher model is properly frozen\n",
    "        if hasattr(self, 'teacher_model'):\n",
    "            LOGGER.info(\"Making sure teacher model parameters are frozen\")\n",
    "            for param in self.teacher_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Check if the teacher model is in the student model\n",
    "        print(f\"self.teacher_model.parameters() {self.teacher_model.parameters()}\")\n",
    "        if hasattr(self.model, 'teacher_model') and self.model.teacher_model is not None:\n",
    "            LOGGER.info(\"Making sure teacher model in student is frozen\")\n",
    "            for param in self.teacher_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def log_distillation_metrics(self, trainer):\n",
    "        \"\"\"Log distillation metrics at the end of each epoch.\"\"\"\n",
    "        if hasattr(self.model, 'criterion') and hasattr(self.model.criterion, 'epoch_metrics'):\n",
    "            metrics = self.model.criterion.epoch_metrics\n",
    "            if metrics[\"count\"] > 0:\n",
    "                avg_combined = metrics[\"combined_loss\"] / metrics[\"count\"]\n",
    "                avg_detection = metrics[\"detection_loss\"] / metrics[\"count\"]\n",
    "                avg_distill = metrics[\"distillation_loss\"] / metrics[\"count\"]\n",
    "                \n",
    "                wandb.log({\n",
    "                    \"distill/avg_combined_loss\": avg_combined,\n",
    "                    \"distill/avg_detection_loss\": avg_detection,\n",
    "                    \"distill/avg_distillation_loss\": avg_distill,\n",
    "                    \"epoch\": self.epoch\n",
    "                })\n",
    "                \n",
    "                # Reset metrics for next epoch\n",
    "                self.model.criterion.epoch_metrics = {\n",
    "                    \"combined_loss\": 0.0, \n",
    "                    \"detection_loss\": 0.0, \n",
    "                    \"distillation_loss\": 0.0, \n",
    "                    \"count\": 0\n",
    "                }   \n",
    "    # def log_training_metrics(self, trainer):\n",
    "    #     \"\"\"Log training metrics at the end of each epoch.\"\"\"\n",
    "    #     if wandb.run is not None:\n",
    "    #         # The trainer already tracks losses\n",
    "    #         training_metrics = {\n",
    "    #             \"box\": self.loss_items[0].item(),\n",
    "    #             \"cls\": self.loss_items[1].item(),\n",
    "    #             \"dfl\": self.loss_items[2].item() if len(self.loss_items) > 2 else 0,\n",
    "    #             \"loss\": self.loss.item(),\n",
    "    #         }\n",
    "    #         wandb.log(training_metrics, step=self.epoch)\n",
    "            # self.model.criterion.log_epoch_metrics(self.epoch)\n",
    "    \n",
    "    # def log_validation_metrics(self, trainer):\n",
    "    #     \"\"\"Log validation metrics including teacher model performance.\"\"\"\n",
    "    #     if wandb.run is not None:\n",
    "    #         # Student metrics\n",
    "    #         student_metrics = {k: v for k, v in self.metrics.items()}\n",
    "            \n",
    "    #         # Teacher metrics (if you're validating the teacher model too)\n",
    "    #         teacher_metrics = {}\n",
    "    #         if hasattr(self, 'teacher_metrics'):\n",
    "    #             teacher_metrics = {f\"val/teacher_{k}\": v for k, v in self.teacher_metrics.items()}\n",
    "            \n",
    "    #         # Log everything together with epoch\n",
    "    #         wandb.log({\n",
    "    #             **student_metrics,\n",
    "    #             **teacher_metrics\n",
    "    #         }, step=self.epoch)\n",
    "       \n",
    "    # def log_teacher_metrics(self, trainer):\n",
    "    #     \"\"\"Log teacher model performance metrics during validation.\"\"\"\n",
    "    #     if not hasattr(self, 'teacher_validator'):\n",
    "    #         # Create a validator for the teacher model\n",
    "    #         self.teacher_validator = MergedClassDetectionValidator(\n",
    "    #             dataloader=self.test_loader,\n",
    "    #             save_dir=self.save_dir / 'teacher_val',\n",
    "    #             args=copy(self.args)\n",
    "    #         )\n",
    "        \n",
    "    #     # Run validation on teacher model\n",
    "    #     LOGGER.info(\"\\n--- Evaluating Teacher Model ---\")\n",
    "    #     teacher_metrics = self.teacher_validator(model=self.teacher_model)\n",
    "        \n",
    "    #     # Log metrics to wandb\n",
    "    #     if wandb.run is not None:\n",
    "    #         teacher_log = {f\"teacher/{k}\": v for k, v in teacher_metrics.items()}\n",
    "    #         student_log = {f\"student/{k}\": v for k, v in self.metrics.items()}\n",
    "            \n",
    "    #         # Calculate improvement/degradation for key metrics\n",
    "    #         for k in ['mAP50', 'mAP50-95', 'precision', 'recall']:\n",
    "    #             if k in teacher_metrics and k in self.metrics:\n",
    "    #                 diff = self.metrics[k] - teacher_metrics[k]\n",
    "    #                 teacher_log[f\"diff/{k}\"] = diff\n",
    "            \n",
    "    #         wandb.log({**teacher_log, **student_log})\n",
    "            \n",
    "    #         # Create a comparison table\n",
    "    #         comparison_data = []\n",
    "    #         for k in ['mAP50', 'mAP50-95', 'precision', 'recall']:\n",
    "    #             if k in teacher_metrics and k in self.metrics:\n",
    "    #                 comparison_data.append([\n",
    "    #                     k, \n",
    "    #                     teacher_metrics[k], \n",
    "    #                     self.metrics[k], \n",
    "    #                     self.metrics[k] - teacher_metrics[k]\n",
    "    #                 ])\n",
    "            \n",
    "    #         comparison_table = wandb.Table(\n",
    "    #             data=comparison_data,\n",
    "    #             columns=[\"Metric\", \"Teacher\", \"Student\", \"Difference\"]\n",
    "    #         )\n",
    "    #         wandb.log({\"comparison\": comparison_table}, step=self.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = get_wandb_key()\n",
    "settings.update({\"wandb\": True})\n",
    "wandb.login(key=key, relogin=True)\n",
    "\n",
    "# wandb_start(key, config[\"train\"], config[\"wandb\"])\n",
    "\n",
    "trial_config = config.copy()\n",
    "\n",
    "trial_config[\"train\"].update({\n",
    "    \"model\": \"yolo12n.pt\",\n",
    "    \"pretrained\": True,\n",
    "    \"imgsz\": 640,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"lr0\": 0.005,\n",
    "    \"lrf\": 0.001,\n",
    "    \"momentum\": 0.937,\n",
    "    \"warmup_epochs\": 20,\n",
    "    \"patience\": 10,\n",
    "    \"batch\": 16,\n",
    "    \"workers\": 4,\n",
    "    \"box\": 3.5,\n",
    "    \"cls\": 0.3,\n",
    "    \"dfl\": 1,\n",
    "    \"cos_lr\": False,\n",
    "})\n",
    "\n",
    "run = wandb.init(\n",
    "    project=trial_config[\"wandb\"][\"project\"], \n",
    "    group=trial_config[\"wandb\"][\"group\"]\n",
    ")\n",
    "wandb.log(trial_config[\"train\"])\n",
    "\n",
    "teacher_artifact = run.use_artifact('francescoperagine-universit-degli-studi-di-bari-aldo-moro/EyeInTheSky_merged/run_uao1xs95_model:v0', type='model')\n",
    "teacher_artifact_dir = teacher_artifact.download()\n",
    "teacher_model = f\"{teacher_artifact_dir}/best.pt\"\n",
    "\n",
    "trial_config[\"distillation\"].update({\n",
    "    \"teacher_model\": teacher_model\n",
    "})\n",
    "\n",
    "DistillationTrainer.set_config(trial_config[\"distillation\"])\n",
    "trainer = DistillationTrainer(overrides=trial_config[\"train\"])\n",
    "\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log final results\n",
    "final_metrics = {f\"final/{k}\": v for k, v in results.items()}\n",
    "wandb.log(final_metrics)\n",
    "\n",
    "# Run final validation\n",
    "LOGGER.info(\"Running final validation on student model...\")\n",
    "student_model = YOLO(str(trainer.best))  # Load best model\n",
    "student_test_results = student_model.val(\n",
    "    validator=MergedClassDetectionValidator,\n",
    "    **config['val']\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "wandb.finish()\n",
    "clear_cache()\n",
    "# remove_models()\n",
    "\n",
    "print(f\"Training completed. Results: {results}\")\n",
    "print(f\"Student test results: {student_test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "\n",
    "# def remove_models():\n",
    "#     pt_files = glob.glob(\"*.pt\")\n",
    "#     print(\"Files to be removed:\", pt_files)\n",
    "\n",
    "#     for file in pt_files:\n",
    "#         os.remove(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
