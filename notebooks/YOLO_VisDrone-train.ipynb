{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['YOLO_VERBOSE'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2630,
     "status": "ok",
     "timestamp": 1740856788765,
     "user": {
      "displayName": "Francesco",
      "userId": "17757392889991151115"
     },
     "user_tz": -60
    },
    "id": "cFCI92wYdpJt",
    "outputId": "03726593-b513-4d1f-a49a-d60bcad195b4"
   },
   "outputs": [],
   "source": [
    "%pip install loguru==0.7.3 python-dotenv==1.0.1 PyYAML==6.0.2 torch==2.5.1 tqdm==4.67.1 typer==0.15.1 matplotlib==3.10.0 pyarrow==18.1.0 setuptools==75.1.0 protobuf==4.25.3 ultralytics==8.3.94 ray==2.43.0 albumentations==2.0.5 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h04mIHIKdUZr"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO, settings\n",
    "import gc\n",
    "import json\n",
    "import locale\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer, DetectionValidator\n",
    "from ultralytics.utils import colorstr, LOGGER\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "# locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "config_data = \"\"\"\n",
    "wandb:\n",
    "  project: \"EyeInTheSky_merged\"\n",
    "  group: \"train\"\n",
    "data: \"VisDrone.yaml\"\n",
    "# k_samples: 3\n",
    "iterations: 1\n",
    "train:\n",
    "  project: \"EyeInTheSky\"\n",
    "  data: \"VisDrone.yaml\"\n",
    "  pretrained: True\n",
    "  patience: 5\n",
    "  task: detect\n",
    "  epochs: 500\n",
    "  seed: 42\n",
    "  plots: True\n",
    "  exist_ok: False\n",
    "  save: True\n",
    "  save_period: 10\n",
    "  val: True\n",
    "  warmup_epochs: 10\n",
    "  visualize: True\n",
    "  show: True\n",
    "  single_cls: False\n",
    "  rect: False\n",
    "  resume: False\n",
    "  fraction: 1.0\n",
    "  freeze: None\n",
    "  cache: False\n",
    "  verbose: False\n",
    "  amp: True\n",
    "val:\n",
    "  project: \"EyeInTheSky\"\n",
    "  half: True\n",
    "  conf: 0.25\n",
    "  iou: 0.6\n",
    "  split: \"test\"\n",
    "  rect: True\n",
    "  plots: True\n",
    "  visualize: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device\n",
    "\n",
    "def get_device() -> str:\n",
    "    try:\n",
    "        return 0 if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting device: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U8n6O74dUZx"
   },
   "outputs": [],
   "source": [
    "# Load config\n",
    "\n",
    "# config = Config.load(\"../config/config.yaml\")\n",
    "config = yaml.safe_load(config_data)\n",
    "config[\"train\"].update({\"device\" : get_device()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Wandb key\n",
    "\n",
    "def get_wandb_key_colab() -> str:\n",
    "    try:\n",
    "        from google.colab import userdata # type: ignore\n",
    "\n",
    "        if userdata.get(\"WANDB_API_KEY\") is not None:\n",
    "            return userdata.get(\"WANDB_API_KEY\")\n",
    "        else:\n",
    "            raise ValueError(\"No WANDB key found\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_wandb_env(path: Path) -> str:\n",
    "    try:\n",
    "        from dotenv import dotenv_values # type: ignore\n",
    "\n",
    "        \"\"\"Get W&B API key from Colab userdata or environment variable\"\"\"\n",
    "\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Could not find .env file at {path}\")\n",
    "\n",
    "        print(f\"Loading secrets from {path}\")\n",
    "\n",
    "        secrets = dotenv_values(path)\n",
    "        print(f\"Found keys: {list(secrets.keys())}\")\n",
    "\n",
    "        if \"WANDB_API_KEY\" not in secrets:\n",
    "            raise KeyError(f\"WANDB_API_KEY not found in {path}. Available keys: {list(secrets.keys())}\")\n",
    "\n",
    "        return secrets['WANDB_API_KEY']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_wandb_key(path: Path = \"../.env\") -> str:\n",
    "    return get_wandb_key_colab() if get_wandb_key_colab() is not None else get_wandb_env(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, Trainer, Validator\n",
    "\n",
    "class VisDroneDataset(YOLODataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for VisDrone that merges pedestrian (0) and people (1) classes.\n",
    "    Handles class remapping at the earliest possible stage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the merged names as a class attribute to be accessible from the trainer\n",
    "    merged_names = {\n",
    "        0: 'persona',\n",
    "        1: 'bicicletta',\n",
    "        2: 'auto',\n",
    "        3: 'furgone',\n",
    "        4: 'camion',\n",
    "        5: 'triciclo',\n",
    "        6: 'triciclo-tendato',\n",
    "        7: 'autobus',\n",
    "        8: 'motociclo'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Initialize parent class with modified kwargs\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Log class mapping\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Using merged classes: {self.merged_names}\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"\n",
    "        Load and process labels with class remapping.\n",
    "        \"\"\"\n",
    "        # Get labels from parent method\n",
    "        labels = super().get_labels()\n",
    "        \n",
    "        # Process statistics\n",
    "        people_count = 0\n",
    "        shifted_count = 0\n",
    "        \n",
    "        # Process labels to merge classes\n",
    "        for i in range(len(labels)):\n",
    "            cls = labels[i]['cls']\n",
    "            \n",
    "            if len(cls) > 0:\n",
    "                # Count 'people' instances\n",
    "                people_mask = cls == 1\n",
    "                people_count += np.sum(people_mask)\n",
    "                \n",
    "                # Merge class 1 (people) into class 0 (pedestrian -> person)\n",
    "                cls[people_mask] = 0\n",
    "                \n",
    "                # Shift classes > 1 down by 1\n",
    "                gt1_mask = cls > 1\n",
    "                shifted_count += np.sum(gt1_mask)\n",
    "                cls[gt1_mask] -= 1\n",
    "                \n",
    "                # Store modified labels\n",
    "                labels[i]['cls'] = cls\n",
    "        \n",
    "        # Now set correct class count and names for training\n",
    "        if hasattr(self, 'data'):\n",
    "            # Update names and class count\n",
    "            self.data['names'] = self.merged_names\n",
    "            self.data['nc'] = len(self.merged_names)\n",
    "        \n",
    "        # Log statistics\n",
    "        person_count = sum(np.sum(label['cls'] == 0) for label in labels)\n",
    "        LOGGER.info(f\"\\n{colorstr('VisDroneDataset:')} Remapped {people_count} 'people' instances to {self.merged_names[0]}\")\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Total 'persona' instances after merge: {person_count}\")\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Shifted {shifted_count} instances of other classes\")\n",
    "        \n",
    "        return labels\n",
    "\n",
    "class MergedClassDetectionTrainer(DetectionTrainer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Custom trainer that uses VisDroneDataset for merged class training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_dataset(self, img_path, mode=\"train\", batch=None):\n",
    "        \"\"\"Build custom VisDroneDataset.\"\"\"\n",
    "        return VisDroneDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch or self.batch_size,\n",
    "            augment=mode == \"train\",\n",
    "            hyp=self.args,\n",
    "            rect=self.args.rect if mode == \"train\" else True,\n",
    "            cache=self.args.cache or None,\n",
    "            single_cls=self.args.single_cls,\n",
    "            stride=self.stride,\n",
    "            pad=0.0 if mode == \"train\" else 0.5,\n",
    "            prefix=colorstr(f\"{mode}: \"),\n",
    "            task=self.args.task,\n",
    "            classes=None,\n",
    "            data=self.data,\n",
    "            fraction=self.args.fraction if mode == \"train\" else 1.0,\n",
    "        )\n",
    "    \n",
    "    def set_model_attributes(self):\n",
    "        \"\"\"Update model attributes for merged classes.\"\"\"\n",
    "        # First call parent method to set standard attributes\n",
    "        super().set_model_attributes()\n",
    "        \n",
    "        # Then update model with the merged class names\n",
    "        if hasattr(self.model, 'names'):\n",
    "            # Use the merged names directly from the dataset class\n",
    "            self.model.names = VisDroneDataset.merged_names\n",
    "            self.model.nc = len(VisDroneDataset.merged_names)\n",
    "            \n",
    "            # Also update data dictionary\n",
    "            if hasattr(self, 'data'):\n",
    "                self.data['names'] = VisDroneDataset.merged_names\n",
    "                self.data['nc'] = len(VisDroneDataset.merged_names)\n",
    "\n",
    "class MergedClassDetectionValidator(DetectionValidator):\n",
    "    \"\"\"\n",
    "    Custom validator that uses VisDroneDataset for validation/testing with merged classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_dataset(self, img_path, mode=\"val\", batch=None):\n",
    "        \"\"\"Build custom VisDroneDataset for validation.\"\"\"\n",
    "        return VisDroneDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch or self.args.batch,\n",
    "            augment=False,\n",
    "            hyp=self.args,\n",
    "            rect=True,\n",
    "            cache=None,\n",
    "            single_cls=self.args.single_cls,\n",
    "            stride=self.stride,\n",
    "            pad=0.5,\n",
    "            prefix=colorstr(f\"{mode}: \"),\n",
    "            task=self.args.task,\n",
    "            classes=self.args.classes,\n",
    "            data=self.data,\n",
    "        )\n",
    "    \n",
    "    def set_model_attributes(self):\n",
    "        \"\"\"Update model attributes for merged classes if using a PyTorch model.\"\"\"\n",
    "        super().set_model_attributes()\n",
    "        \n",
    "        # Update model names if it's a PyTorch model (not for exported models)\n",
    "        if hasattr(self.model, 'names') and hasattr(self.model, 'model'):\n",
    "            self.model.names = VisDroneDataset.merged_names\n",
    "            if hasattr(self.data, 'names'):\n",
    "                self.data['names'] = VisDroneDataset.merged_names\n",
    "                self.data['nc'] = len(VisDroneDataset.merged_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdA0A5b3Eli2"
   },
   "outputs": [],
   "source": [
    "# # Load top k samples from the top 15 configurations by fitness\n",
    "\n",
    "# k = config[\"k_samples\"]\n",
    "# top_n = 15  # Number of top configurations to consider\n",
    "\n",
    "# csv_path = \"../data/processed/wandb_export_2025-03-05T10_24_46.923+01_00.csv\"\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Calculate fitness score\n",
    "# df['fitness'] = df['metrics/mAP50(B)'] * 0.1 + df['metrics/mAP50-95(B)'] * 0.9\n",
    "# df = df.dropna(subset=['fitness'])\n",
    "\n",
    "# # First, sort by fitness to get the top N configurations\n",
    "# df_top = df.sort_values(by='fitness', ascending=False).head(top_n)\n",
    "\n",
    "# # Then sample k configurations from these top performers\n",
    "# df_sampled = df_top.sample(n=min(k, len(df_top)))\n",
    "\n",
    "# columns_to_show = ['fitness', 'metrics/mAP50-95(B)', 'metrics/mAP50(B)', \n",
    "#                    'metrics/precision(B)', 'metrics/recall(B)', 'optimizer', \n",
    "#                    'lr0', 'lrf', 'momentum', 'weight_decay', 'box', 'cls', 'dfl']\n",
    "\n",
    "# df_k_sampled = df_sampled[columns_to_show].reset_index(drop=True)\n",
    "# print(f\"Sampled {k} configs from top {top_n} by Fitness Score:\")\n",
    "\n",
    "# display(df_k_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache\n",
    "\n",
    "def clear_cache():\n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "\n",
    "def save_results(dir, name, results):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_path = f\"{dir}/{name}_{timestamp}.json\"\n",
    "\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4, default=str)\n",
    "    \n",
    "    print(f\"{name} results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optimizer_step\n",
    "\n",
    "# def optimizer_step(trainer):\n",
    "#     \"\"\"\n",
    "#     Custom callback to implement cyclical learning rate schedule with parameter-specific learning rates.\n",
    "#     This properly accounts for the Ultralytics optimizer parameter grouping where:\n",
    "#     - Group 0: Weights with decay (not in BatchNorm layers)\n",
    "#     - Group 1: BatchNorm weights (no decay)\n",
    "#     - Group 2: Bias parameters (no decay)\n",
    "#     \"\"\"\n",
    "#     # Only proceed if cyclic learning rate is enabled in config\n",
    "#     if not trainer.data.get('cyclic_lr', {}).get('enabled', False):\n",
    "#         return\n",
    "    \n",
    "#     # Get current epoch\n",
    "#     epoch = trainer.epoch\n",
    "    \n",
    "#     # Get cyclic LR config parameters from the data dictionary\n",
    "#     cyclic_config = trainer.data.get('cyclic_lr', {})\n",
    "#     cycle_size = cyclic_config.get('cycle_size', 80)\n",
    "#     lr_min_factor = cyclic_config.get('min_factor', 0.1)\n",
    "#     cycle_decay = cyclic_config.get('decay', 0.85)\n",
    "    \n",
    "#     # Get initial learning rate from training args\n",
    "#     initial_lr = trainer.args.lr0\n",
    "    \n",
    "#     # Calculate current cycle (1-based)\n",
    "#     cycle = math.floor(epoch / cycle_size) + 1\n",
    "    \n",
    "#     # Calculate position within current cycle (0 to 1)\n",
    "#     cycle_position = (epoch % cycle_size) / cycle_size\n",
    "    \n",
    "#     # Apply decay based on cycle number\n",
    "#     cycle_decay_factor = cycle_decay ** (cycle - 1)\n",
    "    \n",
    "#     # Calculate LR factor using cosine annealing within each cycle\n",
    "#     cosine_factor = 0.5 * (1 + math.cos(math.pi * cycle_position))\n",
    "#     lr_range = 1.0 - lr_min_factor\n",
    "#     lr_factor = (lr_min_factor + lr_range * cosine_factor) * cycle_decay_factor\n",
    "    \n",
    "#     # Base learning rate for this cycle\n",
    "#     base_lr = initial_lr * lr_factor\n",
    "    \n",
    "#     # Set parameter-specific multipliers according to YOLO conventions\n",
    "#     # Group 0: Weights with decay - standard learning rate\n",
    "#     # Group 1: BatchNorm weights - can use higher learning rate\n",
    "#     # Group 2: Bias parameters - typically higher learning rate\n",
    "#     weight_decay_lr = base_lr * 1.0    # Standard rate for weights with decay\n",
    "#     bn_lr = base_lr * 1.5              # Higher rate for BatchNorm (no decay)\n",
    "#     bias_lr = base_lr * 2.0            # Even higher rate for bias terms\n",
    "    \n",
    "#     learning_rates = [weight_decay_lr, bn_lr, bias_lr]\n",
    "#     group_names = [\"Weights (with decay)\", \"BatchNorm weights\", \"Bias parameters\"]\n",
    "    \n",
    "#     # Log parameter group information periodically\n",
    "#     if epoch < 5 or epoch % 10 == 0:\n",
    "#         LOGGER.info(f\"\\nEpoch {epoch}: Examining optimizer parameter groups:\")\n",
    "#         for i, pg in enumerate(trainer.optimizer.param_groups):\n",
    "#             params_count = sum(p.numel() for p in pg['params'] if p.requires_grad)\n",
    "#             current_lr = pg.get('lr', 'unknown')\n",
    "#             LOGGER.info(f\"  Group {i} ({group_names[i]}): {params_count} parameters, current lr: {current_lr}\")\n",
    "    \n",
    "#     # Print the calculated learning rates\n",
    "#     if epoch < 5 or epoch % 10 == 0:\n",
    "#         LOGGER.info(f\"  Calculated LRs: {[f'{lr:.6f}' for lr in learning_rates]}\")\n",
    "    \n",
    "#     # Apply learning rates to each parameter group\n",
    "#     for i, param_group in enumerate(trainer.optimizer.param_groups):\n",
    "#         if i < len(learning_rates):\n",
    "#             # Store previous learning rate for comparison\n",
    "#             prev_lr = param_group.get('lr', 'unknown')\n",
    "            \n",
    "#             # Set new learning rate\n",
    "#             param_group['lr'] = learning_rates[i]\n",
    "            \n",
    "#             # Verify change (in early epochs or periodically)\n",
    "#             if epoch < 5 or epoch % 10 == 0:\n",
    "#                 LOGGER.info(f\"  Group {i} ({group_names[i]}): LR changed from {prev_lr} to {learning_rates[i]}\")\n",
    "    \n",
    "#     # Log to wandb if available\n",
    "#     if wandb.run is not None:\n",
    "#         wandb.log({\n",
    "#             \"lr/weights_decay\": learning_rates[0],\n",
    "#             \"lr/bn_weights\": learning_rates[1],\n",
    "#             \"lr/bias\": learning_rates[2],\n",
    "#             \"lr/cycle_factor\": lr_factor,\n",
    "#             \"lr/cycle\": cycle,\n",
    "#             \"lr/cycle_position\": cycle_position,\n",
    "#             \"epoch\": epoch\n",
    "#         })\n",
    "    \n",
    "#     # Force an explicit print to ensure output is visible\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triangular_lr_callback(base_lr=0.001, max_lr=0.01, cycle_length=1000, group_scalers=None):\n",
    "    \"\"\"\n",
    "    Creates a triangular learning rate callback.\n",
    "    \n",
    "    Args:\n",
    "        base_lr (float): The minimum learning rate.\n",
    "        max_lr (float): The maximum learning rate.\n",
    "        cycle_length (int): Number of batches to complete one cycle.\n",
    "        group_scalers (list, optional): Multipliers for each parameter group \n",
    "            (order: [weight decay, batchnorm (no decay), bias]). Default is [1.0, 1.0, 1.5].\n",
    "    \n",
    "    Returns:\n",
    "        function: A callback function to be registered with model.add_callback().\n",
    "    \"\"\"\n",
    "    if group_scalers is None:\n",
    "        group_scalers = [1.0, 1.0, 1.5]\n",
    "    \n",
    "    def triangular_lr_callback(trainer, **kwargs):\n",
    "        # Initialize the iteration counter if not already present.\n",
    "        if not hasattr(trainer, 'lr_iteration'):\n",
    "            trainer.lr_iteration = 0\n",
    "        trainer.lr_iteration += 1\n",
    "\n",
    "        # Calculate current position in the cycle.\n",
    "        cycle_iter = trainer.lr_iteration % cycle_length\n",
    "        half_cycle = cycle_length / 2\n",
    "\n",
    "        # Determine scaling factor: increasing in first half, decreasing in second half.\n",
    "        if cycle_iter <= half_cycle:\n",
    "            factor = cycle_iter / half_cycle\n",
    "        else:\n",
    "            factor = (cycle_length - cycle_iter) / half_cycle\n",
    "\n",
    "        # Compute the new base learning rate according to the triangular policy.\n",
    "        new_lr = base_lr + (max_lr - base_lr) * factor\n",
    "\n",
    "        # Update the learning rate for each optimizer parameter group.\n",
    "        for i, param_group in enumerate(trainer.optimizer.param_groups):\n",
    "            scaler = group_scalers[i] if i < len(group_scalers) else 1.0\n",
    "            param_group['lr'] = new_lr * scaler\n",
    "\n",
    "        # Optional: Uncomment the next line to print the learning rate for debugging.\n",
    "        # print(f\"Iteration {trainer.lr_iteration}: new base LR = {new_lr:.6f}\")\n",
    "\n",
    "    return triangular_lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training and validation\n",
    "\n",
    "def start(model: YOLO, config):\n",
    "    train_results = model.train(\n",
    "        trainer=MergedClassDetectionTrainer,\n",
    "        **config['train']\n",
    "        )\n",
    "\n",
    "    test_results = model.val(\n",
    "        validator=MergedClassDetectionValidator,\n",
    "        **config['val']\n",
    "        )\n",
    "\n",
    "    return train_results, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove models\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def remove_models():\n",
    "    pt_files = glob.glob(\"*.pt\")\n",
    "    print(\"Files to be removed:\", pt_files)\n",
    "\n",
    "    for file in pt_files:\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb\n",
    "\n",
    "def wandb_start(key, run_config, wandb_config):\n",
    "    settings.update({\"wandb\": True})\n",
    "\n",
    "    wandb.login(key=key, relogin=True)\n",
    "    wandb.init(project=wandb_config[\"project\"], group=wandb_config[\"group\"])\n",
    "    wandb.log(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "key = get_wandb_key()\n",
    "settings.update({\"wandb\": True})\n",
    "wandb.login(key=key, relogin=True)\n",
    "\n",
    "for i in range(config[\"iterations\"]):\n",
    "\n",
    "    trial = config.copy()\n",
    "    # trial.setdefault('cyclic_lr', {})['enabled'] = True\n",
    "    \n",
    "    trial[\"train\"].update({\n",
    "        \"model\": \"yolo12l.pt\",\n",
    "        \"pretrained\": True,\n",
    "        \"imgsz\": 1280,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"lr0\": 0.001,\n",
    "        \"lrf\": 0.01,\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"patience\": 5,\n",
    "        \"batch\": 2,\n",
    "        \"workers\": 2,\n",
    "        \"momentum\": 0.937,\n",
    "        \"box\": 3.5,\n",
    "        \"cls\": 0.3,\n",
    "        \"dfl\": 1,\n",
    "        \"cos_lr\": False,\n",
    "    })\n",
    "\n",
    "    # trial.setdefault('cyclic_lr',{\n",
    "    #     \"base_lr\": 0.001,\n",
    "    #     \"max_lr\": 0.01,\n",
    "    #     \"cycle_size\": 1000,   \n",
    "    # })\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=trial[\"wandb\"][\"project\"], \n",
    "        group=trial[\"wandb\"][\"group\"]\n",
    "    )\n",
    "    wandb.log(trial[\"train\"])\n",
    "\n",
    "    model = YOLO(trial[\"train\"][\"model\"])\n",
    "\n",
    "    # triangular_lr_cb = create_triangular_lr_callback(\n",
    "    #         base_lr=trial[\"cyclic_lr\"][\"base_lr\"], \n",
    "    #         max_lr=trial[\"cyclic_lr\"][\"max_lr\"], \n",
    "    #         cycle_length=trial[\"cyclic_lr\"][\"cycle_size\"]\n",
    "    #     )\n",
    "    # model.add_callback(\"on_train_batch_end\", triangular_lr_cb)\n",
    "\n",
    "    wandb_start(key, trial[\"train\"], trial[\"wandb\"])\n",
    "    # wandb.log(trial[\"cyclic_lr\"])\n",
    "\n",
    "    train_results, test_results = start(model, trial)\n",
    "\n",
    "    save_results(\"../data/processed\", \"train\", train_results)\n",
    "    save_results(\"../data/processed\", \"test\", test_results)\n",
    "\n",
    "    clear_cache()\n",
    "    remove_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "\n",
    "# resume_config = config.copy()\n",
    "# del resume_config[\"val\"][\"name\"]\n",
    "# resume_config[\"train\"].update({\n",
    "#     \"epochs\": 300, \n",
    "#     \"device\": 0,\n",
    "#     \"warmup_epochs\": 0,\n",
    "#     \"optimizer\": \"AdamW\",\n",
    "# })\n",
    "# resume_config.update(df_train.iloc[0])\n",
    "# resume_config"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
