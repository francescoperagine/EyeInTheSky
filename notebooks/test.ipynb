{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2630,
     "status": "ok",
     "timestamp": 1740856788765,
     "user": {
      "displayName": "Francesco",
      "userId": "17757392889991151115"
     },
     "user_tz": -60
    },
    "id": "cFCI92wYdpJt",
    "outputId": "03726593-b513-4d1f-a49a-d60bcad195b4"
   },
   "outputs": [],
   "source": [
    "%pip install loguru==0.7.3 python-dotenv==1.0.1 PyYAML==6.0.2 torch==2.5.1 tqdm==4.67.1 typer==0.15.1 matplotlib==3.10.0 pyarrow==18.1.0 setuptools==75.1.0 protobuf==4.25.3 ultralytics==8.3.107 ray==2.43.0 albumentations==2.0.5 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h04mIHIKdUZr"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image \n",
    "from ultralytics import YOLO, settings\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer, DetectionValidator\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ultralytics.utils import colorstr, LOGGER\n",
    "import glob\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "settings.update({\"wandb\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def _get_wandb_key_colab() -> str:\n",
    "    try:\n",
    "        from google.colab import userdata # type: ignore\n",
    "\n",
    "        if userdata.get(\"WANDB_API_KEY\") is not None:\n",
    "            return userdata.get(\"WANDB_API_KEY\")\n",
    "        else:\n",
    "            raise ValueError(\"No WANDB key found\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def _get_wandb_env(path: Path) -> str:\n",
    "    try:\n",
    "        from dotenv import dotenv_values # type: ignore\n",
    "\n",
    "        \"\"\"Get W&B API key from Colab userdata or environment variable\"\"\"\n",
    "\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Could not find .env file at {path}\")\n",
    "\n",
    "        print(f\"Loading secrets from {path}\")\n",
    "\n",
    "        secrets = dotenv_values(path)\n",
    "        print(f\"Found keys: {list(secrets.keys())}\")\n",
    "\n",
    "        if \"WANDB_API_KEY\" not in secrets:\n",
    "            raise KeyError(f\"WANDB_API_KEY not found in {path}. Available keys: {list(secrets.keys())}\")\n",
    "\n",
    "        return secrets['WANDB_API_KEY']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_wandb_key(path: Path = \"../.env\") -> str:\n",
    "    return _get_wandb_key_colab() if _get_wandb_key_colab() is not None else _get_wandb_env(path)\n",
    "\n",
    "def get_device() -> str:\n",
    "    try:\n",
    "        return 0 if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting device: {e}\")\n",
    "\n",
    "def load_config(config_file: str) -> dict:\n",
    "    \"\"\"Load and return configuration from YAML file.\"\"\"\n",
    "    with open(config_file, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def remove_models():\n",
    "    pt_files = glob.glob(\"*.pt\")\n",
    "    print(\"Files to be removed:\", pt_files)\n",
    "\n",
    "    for file in pt_files:\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.cwd().parents[0]\n",
    "WANDB_REPORT_PATH = os.path.join(ROOT_PATH, \"reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "config_data = \"\"\"\n",
    "wandb:\n",
    "  project: \"EyeInTheSky_test\"\n",
    "  dir: \"reports\"\n",
    "model: \"<wandb_artifact_source>\" # wandb artifact to download\n",
    "val:\n",
    "  project: \"EyeInTheSky\"\n",
    "  data: \"VisDrone.yaml\"\n",
    "  name: \"test\" \n",
    "  half: True\n",
    "  conf: 0.15\n",
    "  iou: 0.6\n",
    "  split: \"test\"\n",
    "  rect: True\n",
    "  plots: True\n",
    "  visualize: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U8n6O74dUZx"
   },
   "outputs": [],
   "source": [
    "# Load config\n",
    "\n",
    "# config = Config.load(\"../config/config.yaml\")\n",
    "config = yaml.safe_load(config_data)\n",
    "config[\"val\"].update({\"device\" : get_device()})\n",
    "\n",
    "config[\"wandb\"].update({\n",
    "    \"dir\": WANDB_REPORT_PATH,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, Trainer, Validator\n",
    "\n",
    "class VisDroneDataset(YOLODataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for VisDrone that merges pedestrian (0) and people (1) classes.\n",
    "    \n",
    "    This dataset handler performs class remapping at the earliest stage of the pipeline\n",
    "    by combining pedestrian and people into a single 'persona' class and shifting all \n",
    "    other class indices down by one. The merged class mapping is stored as a class \n",
    "    attribute for access during training and validation.\n",
    "    \n",
    "    The remapping happens in the get_labels() method which modifies the label tensors\n",
    "    directly, ensuring all downstream processing uses the merged classes.\n",
    "    \n",
    "    Class attributes:\n",
    "        merged_names (dict): New class mapping after merging pedestrian and people classes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the merged names as a class attribute to be accessible from the trainer\n",
    "    merged_names = {\n",
    "        0: 'persona',\n",
    "        1: 'bicicletta',\n",
    "        2: 'auto',\n",
    "        3: 'furgone',\n",
    "        4: 'camion',\n",
    "        5: 'triciclo',\n",
    "        6: 'triciclo-tendato',\n",
    "        7: 'autobus',\n",
    "        8: 'motociclo'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Initialize parent class with modified kwargs\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Log class mapping\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Using merged classes: {self.merged_names}\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"\n",
    "        Load and process labels with class remapping.\n",
    "        \"\"\"\n",
    "        # Get labels from parent method\n",
    "        labels = super().get_labels()\n",
    "        \n",
    "        # Process statistics\n",
    "        people_count = 0\n",
    "        shifted_count = 0\n",
    "        \n",
    "        # Process labels to merge classes\n",
    "        for i in range(len(labels)):\n",
    "            cls = labels[i]['cls']\n",
    "            \n",
    "            if len(cls) > 0:\n",
    "                # Count 'people' instances\n",
    "                people_mask = cls == 1\n",
    "                people_count += np.sum(people_mask)\n",
    "                \n",
    "                # Merge class 1 (people) into class 0 (pedestrian -> person)\n",
    "                cls[people_mask] = 0\n",
    "                \n",
    "                # Shift classes > 1 down by 1\n",
    "                gt1_mask = cls > 1\n",
    "                shifted_count += np.sum(gt1_mask)\n",
    "                cls[gt1_mask] -= 1\n",
    "                \n",
    "                # Store modified labels\n",
    "                labels[i]['cls'] = cls\n",
    "        \n",
    "        # Now set correct class count and names for training\n",
    "        if hasattr(self, 'data'):\n",
    "            # Update names and class count\n",
    "            self.data['names'] = self.merged_names\n",
    "            self.data['nc'] = len(self.merged_names)\n",
    "        \n",
    "        # Log statistics\n",
    "        person_count = sum(np.sum(label['cls'] == 0) for label in labels)\n",
    "        LOGGER.info(f\"\\n{colorstr('VisDroneDataset:')} Remapped {people_count} 'people' instances to {self.merged_names[0]}\")\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Total 'persona' instances after merge: {person_count}\")\n",
    "        LOGGER.info(f\"{colorstr('VisDroneDataset:')} Shifted {shifted_count} instances of other classes\")\n",
    "        \n",
    "        return labels\n",
    "\n",
    "class MergedClassDetectionTrainer(DetectionTrainer):\n",
    "    \"\"\"\n",
    "    Custom YOLO trainer that uses the VisDroneDataset with merged classes.\n",
    "    \n",
    "    Extends the standard DetectionTrainer to work with the merged-class dataset.\n",
    "    The key modifications are in build_dataset() to use VisDroneDataset instead of\n",
    "    the default, and in set_model_attributes() to properly update the model's class\n",
    "    names and count to match the merged dataset.\n",
    "    \n",
    "    This ensures that all aspects of training - from data loading to loss calculation -\n",
    "    work consistently with the merged class structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_dataset(self, img_path, mode=\"train\", batch=None):\n",
    "        \"\"\"Build custom VisDroneDataset.\"\"\"\n",
    "        return VisDroneDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch or self.batch_size,\n",
    "            augment=mode == \"train\",\n",
    "            hyp=self.args,\n",
    "            rect=self.args.rect if mode == \"train\" else True,\n",
    "            cache=self.args.cache or None,\n",
    "            single_cls=self.args.single_cls,\n",
    "            stride=self.stride,\n",
    "            pad=0.0 if mode == \"train\" else 0.5,\n",
    "            prefix=colorstr(f\"{mode}: \"),\n",
    "            task=self.args.task,\n",
    "            classes=None,\n",
    "            data=self.data,\n",
    "            fraction=self.args.fraction if mode == \"train\" else 1.0,\n",
    "        )\n",
    "    \n",
    "    def get_model(self, cfg=None, weights=None, verbose=True):\n",
    "        \"\"\"Create and return a DetectionModel.\"\"\"\n",
    "        \n",
    "        model = DetectionModel(\n",
    "            cfg=cfg, \n",
    "            nc=self.data[\"nc\"],\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        model.args = self.args\n",
    "        \n",
    "        if weights:\n",
    "            LOGGER.info(f\"Loading weights into model\")\n",
    "            model.load(weights)\n",
    "            \n",
    "        return model    \n",
    "    \n",
    "    def set_model_attributes(self):\n",
    "        \"\"\"Update model attributes for merged classes.\"\"\"\n",
    "        # First call parent method to set standard attributes\n",
    "        super().set_model_attributes()\n",
    "        \n",
    "        # Then update model with the merged class names\n",
    "        if hasattr(self.model, 'names'):\n",
    "            # Use the merged names directly from the dataset class\n",
    "            self.model.names = VisDroneDataset.merged_names\n",
    "            self.model.nc = len(VisDroneDataset.merged_names)\n",
    "            \n",
    "            # Also update data dictionary\n",
    "            if hasattr(self, 'data'):\n",
    "                self.data['names'] = VisDroneDataset.merged_names\n",
    "                self.data['nc'] = len(VisDroneDataset.merged_names)\n",
    "\n",
    "class MergedClassDetectionValidator(DetectionValidator):\n",
    "    \"\"\"\n",
    "    Custom validator for evaluating models trained on merged VisDrone classes.\n",
    "    \n",
    "    Works in tandem with MergedClassDetectionTrainer to ensure that validation\n",
    "    uses the same class merging as training. The build_dataset() method creates\n",
    "    VisDroneDataset instances for validation, and set_model_attributes() updates\n",
    "    the model's class configuration to match the merged dataset.\n",
    "    \n",
    "    This allows for consistent metrics calculation across training and evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_dataset(self, img_path, mode=\"val\", batch=None):\n",
    "        \"\"\"Build custom VisDroneDataset for validation.\"\"\"\n",
    "        return VisDroneDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch or self.args.batch,\n",
    "            augment=False,\n",
    "            hyp=self.args,\n",
    "            rect=self.args.rect,\n",
    "            cache=None,\n",
    "            single_cls=self.args.single_cls,\n",
    "            stride=self.stride,\n",
    "            pad=0.5,\n",
    "            prefix=colorstr(f\"{mode}: \"),\n",
    "            task=self.args.task,\n",
    "            classes=None,\n",
    "            data=self.data,\n",
    "        )\n",
    "       \n",
    "    def set_model_attributes(self):\n",
    "        \"\"\"Update model attributes for merged classes if using a PyTorch model.\"\"\"\n",
    "        super().set_model_attributes()\n",
    "        \n",
    "        # Then update model with the merged class names\n",
    "        if hasattr(self.model, 'names'):\n",
    "            # Use the merged names directly from the dataset class\n",
    "            self.model.names = VisDroneDataset.merged_names\n",
    "            self.model.nc = len(VisDroneDataset.merged_names)\n",
    "            \n",
    "            # Also update data dictionary\n",
    "            if hasattr(self, 'data'):\n",
    "                self.data['names'] = VisDroneDataset.merged_names\n",
    "                self.data['nc'] = len(VisDroneDataset.merged_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove models\n",
    "\n",
    "def remove_models():\n",
    "    pt_files = glob.glob(\"*.pt\")\n",
    "    print(\"Files to be removed:\", pt_files)\n",
    "\n",
    "    for file in pt_files:\n",
    "        os.remove(file)\n",
    "remove_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB init\n",
    "\n",
    "key = get_wandb_key()\n",
    "wandb.login(key=key, relogin=True)\n",
    "\n",
    "run = wandb.init(\n",
    "    save_code=True,\n",
    "    **config[\"wandb\"],\n",
    ")\n",
    "wandb.log({**config[\"val\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact download \n",
    "\n",
    "artifact = run.use_artifact(config[\"model\"], type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = artifact_dir + \"/best.pt\"\n",
    "model = YOLO(model_file, task=\"detect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(trainer):\n",
    "    \"\"\"\n",
    "    Log the fitness metric to wandb.\n",
    "    \"\"\"\n",
    "    if trainer.fitness is not None and trainer.fitness > 0.0:\n",
    "      metrics_dict = {\n",
    "          **trainer.metrics,\n",
    "          \"metrics/fitness\": trainer.fitness,\n",
    "      }\n",
    "      wandb.log(metrics_dict)\n",
    "      \n",
    "# visualization has commit=false in on_train_epoch_end\n",
    "model.add_callback(\"on_fit_epoch_end\", log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.val(\n",
    "    validator=MergedClassDetectionValidator,\n",
    "    **config['val']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixed_results_dict = {f\"test/{k}\": v for k, v in test_results.results_dict.items()}\n",
    "\n",
    "wandb.init(\n",
    "    id=run.id,\n",
    "    resume=\"must\",\n",
    ")\n",
    "\n",
    "# Prepare all metrics in their respective dictionaries\n",
    "metrics = {\n",
    "    \"test/metrics/fitness\": test_results.fitness,\n",
    "    **prefixed_results_dict\n",
    "}\n",
    "\n",
    "# Add speed metrics\n",
    "for key, value in test_results.speed.items():\n",
    "    metrics[f\"speed/{key}\"] = value\n",
    "\n",
    "# Add class-wise mAP values\n",
    "for i, map_value in enumerate(test_results.maps):\n",
    "    if i in test_results.names:\n",
    "        class_name = test_results.names[i]\n",
    "        metrics[f\"test/metrics/mAP/{class_name}\"] = float(map_value)\n",
    "\n",
    "# Log everything in a single call\n",
    "wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "remove_models()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
